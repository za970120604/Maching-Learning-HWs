{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9YYJzsFZC-B"
      },
      "source": [
        "## HW3: Decision Tree, AdaBoost and Random Forest\n",
        "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
        "\n",
        "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iugBVFXmZC-D"
      },
      "source": [
        "## Question 1\n",
        "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6h7n69sZC-D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Copy and paste your implementations right here to check your result\n",
        "# (Of course you can add your classes not written here)\n",
        "def gini(sequence):\n",
        "  clss = {}\n",
        "  for i in sequence:\n",
        "    if(clss.get(i) == None):\n",
        "      clss[i] = 1\n",
        "    else:\n",
        "      clss[i] += 1\n",
        "  gini = 1\n",
        "  for key in clss.keys():\n",
        "    gini -= clss[key]/len(sequence) * clss[key]/len(sequence)\n",
        "  return gini\n",
        "\n",
        "def entropy(sequence):\n",
        "  clss = {}\n",
        "  for i in sequence:\n",
        "    if(clss.get(i) == None):\n",
        "      clss[i] = 1\n",
        "    else:\n",
        "      clss[i] += 1\n",
        "  entropy = 0\n",
        "  for key in clss.keys():\n",
        "    entropy -= clss[key]/len(sequence) * np.log2(clss[key]/len(sequence))\n",
        "  return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnkKKT5aZC-E"
      },
      "outputs": [],
      "source": [
        "# 1 = class 1,\n",
        "# 2 = class 2\n",
        "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9vnQWopZC-E",
        "outputId": "4295db41-0807-4d34-fd7a-80012ab50b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini of data is  0.4628099173553719\n"
          ]
        }
      ],
      "source": [
        "print(\"Gini of data is \", gini(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSb2ypmbZC-F",
        "outputId": "f165c780-e3ae-450e-bc77-2b1d37ccd479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of data is  0.9456603046006401\n"
          ]
        }
      ],
      "source": [
        "print(\"Entropy of data is \", entropy(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F--0izlyZC-F"
      },
      "source": [
        "## Load data\n",
        "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "ihI-U7t_ZC-G",
        "outputId": "441f84ff-9849-450b-fb22-a083459b8a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20,) (1200, 20) (1200,) (300, 20) (300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
              "0           1583     1          2.1         1  11       0          14    0.7   \n",
              "1            745     1          0.6         1   5       0          35    0.8   \n",
              "2            832     0          0.7         1   2       1          39    0.7   \n",
              "3           1175     1          1.3         0   2       0          19    0.3   \n",
              "4            695     0          0.5         0  18       1          12    0.6   \n",
              "\n",
              "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
              "0        148        7  ...        942      1651  1704    17    13          2   \n",
              "1        102        8  ...         89      1538  2459    14     1         16   \n",
              "2        103        4  ...        125      1504  1799     5     2         11   \n",
              "3        164        7  ...        873      1394  1944     9     4          9   \n",
              "4        196        2  ...       1649      1829  2855    16    13          7   \n",
              "\n",
              "   three_g  touch_screen  wifi  price_range  \n",
              "0        1             0     1            1  \n",
              "1        1             1     0            0  \n",
              "2        1             0     1            0  \n",
              "3        1             1     0            0  \n",
              "4        1             1     1            1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ff7c1a4-977c-460b-a64a-85ca597bad19\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1583</td>\n",
              "      <td>1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0.7</td>\n",
              "      <td>148</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>942</td>\n",
              "      <td>1651</td>\n",
              "      <td>1704</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>745</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0.8</td>\n",
              "      <td>102</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>89</td>\n",
              "      <td>1538</td>\n",
              "      <td>2459</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>832</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>0.7</td>\n",
              "      <td>103</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>125</td>\n",
              "      <td>1504</td>\n",
              "      <td>1799</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1175</td>\n",
              "      <td>1</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0.3</td>\n",
              "      <td>164</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>873</td>\n",
              "      <td>1394</td>\n",
              "      <td>1944</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>695</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0.6</td>\n",
              "      <td>196</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1649</td>\n",
              "      <td>1829</td>\n",
              "      <td>2855</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ff7c1a4-977c-460b-a64a-85ca597bad19')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ff7c1a4-977c-460b-a64a-85ca597bad19 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ff7c1a4-977c-460b-a64a-85ca597bad19');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# %pip install pandas-stubs\n",
        "train_df = pd.read_csv('https://raw.githubusercontent.com/NCTU-VRDL/CS_CS20024/main/HW3/train.csv')\n",
        "val_df = pd.read_csv('https://raw.githubusercontent.com/NCTU-VRDL/CS_CS20024/main/HW3/val.csv')\n",
        "\n",
        "X = train_df.drop(labels=[\"price_range\"], axis=\"columns\")\n",
        "\n",
        "feature_names = X.columns.values\n",
        "X = X.values\n",
        "Y = train_df['price_range'].values\n",
        "\n",
        "X_test = val_df.drop(labels=[\"price_range\"], axis=\"columns\")\n",
        "X_test = X_test.values\n",
        "Y_test = val_df['price_range'].values\n",
        "print(feature_names.shape , X.shape , Y.shape , X_test.shape , Y_test.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HNhCuWMZC-G"
      },
      "source": [
        "## Question 2\n",
        "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
        "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
        "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def unique_vals(rows_till_now , col , interpolated = False): # rows till now : row id till now , a list of ids\n",
        "  global X\n",
        "  uniqs = set()                               # x_data : a list  ; col : a number\n",
        "  if not interpolated:\n",
        "    for row in rows_till_now:\n",
        "      uniqs.add(X[row][col])\n",
        "  else:\n",
        "    uniqs_ = {}\n",
        "    for row in rows_till_now:\n",
        "      val = X[row][col]\n",
        "      if uniqs_.get(val):continue\n",
        "      else:uniqs_[val] = 1\n",
        "    uniqss_ = sorted(uniqs_.keys())\n",
        "    for i in range(len(uniqss_)):\n",
        "      if i == 0 :\n",
        "        uniqs.add(uniqss_[i])\n",
        "      else :\n",
        "        uniqs.add( (uniqss_[i] + uniqss_[i-1]) / 2.0 )\n",
        "  return list(uniqs)\n",
        "\n",
        "def match(input , col , threshold , is_binary): # 對於第col個的feature , 檢驗input值是否符合threshold , 丟入col的原因是為了檢察是不是binary column(either 0 or 1)\n",
        "  if not is_binary:\n",
        "    if input >= threshold: \n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    if input == threshold:\n",
        "      return True\n",
        "    else:\n",
        "      return False \n",
        "\n",
        "def partition(rows_till_now , labels_till_now , col , criteria , is_binary): # 根據資料集中的某一特定col的值是否>=criteria(如果是binary col , >= 變成 ==)來拆分資料集(rows_till_now , labels_till_now)\n",
        "  global X\n",
        "  true_rows = []\n",
        "  false_rows = []\n",
        "\n",
        "  true_labels = []\n",
        "  false_labels = []\n",
        "\n",
        "  for i , row in enumerate(rows_till_now):\n",
        "    if match(X[row][col] , col , criteria , is_binary) :\n",
        "      true_rows.append(row)\n",
        "      true_labels.append(labels_till_now[i])\n",
        "    else :\n",
        "      false_rows.append(row)\n",
        "      false_labels.append(labels_till_now[i])\n",
        "  return true_rows , true_labels , false_rows , false_labels \n",
        "\n",
        "def information_gain(true_rows , true_labels , false_rows , false_labels , current_impurity , criterion): # 根據拆分結果算information gain , 要傳criterion是因為不同棵樹可能有不同的評斷impurity的標準\n",
        "  weight = len(true_rows) / (len(true_rows) + len(false_rows)) * 1.0\n",
        "  info_gain = current_impurity\n",
        "  if criterion == 'gini' :\n",
        "    info_gain -= weight * gini(true_labels) + (1 - weight) * gini(false_labels)\n",
        "  else :\n",
        "    info_gain -= weight * entropy(true_labels) + (1 - weight) * entropy(false_labels)\n",
        "\n",
        "  return info_gain\n",
        "\n",
        "def find_best_split(rows_till_now , labels_till_now , criterion , total_features , random_): # 根據此棵樹給定的impurity criterion , 再資料集中各個col選出一個最好的作為拆分準則\n",
        "  features = []\n",
        "  if random_ == total_features:\n",
        "    for i in range(total_features):\n",
        "      features.append(i)\n",
        "  else :\n",
        "    ll = []\n",
        "    for i in range(total_features):\n",
        "      ll.append(i)\n",
        "    picked = random.sample(ll , random_)\n",
        "    features = picked\n",
        "\n",
        "  current_impurity = 0.0\n",
        "  \n",
        "  if criterion == 'gini':\n",
        "    current_impurity = gini(labels_till_now)\n",
        "  else:\n",
        "    current_impurity = entropy(labels_till_now)\n",
        "\n",
        "  best_gain = 0 \n",
        "  best_feature_id = 0\n",
        "  best_value = 0\n",
        "  best_isbinary = None\n",
        "\n",
        "  for f in features:  # iterate through every feature , i表試著拿第幾個feature做拆分標準\n",
        "    # print(features[i])\n",
        "    vals = unique_vals(rows_till_now , f , False) # 當前這份資料集中 , 第i個feature的所有特殊元素\n",
        "    is_binary = False\n",
        "    if len(vals) == 2:\n",
        "      is_binary = True\n",
        "    for val in vals:\n",
        "      criteria = val\n",
        "      true_rows, true_labels , false_rows , false_labels = partition(rows_till_now , labels_till_now , f , criteria , is_binary)\n",
        "      # print(len(true_rows) , len(false_rows))\n",
        "      if len(true_rows) == 0 or len(false_rows) == 0: # 無用的拆分 , 意即根本沒辦法分群的一個條件組合\n",
        "        # print(f\"split using {features[i]} with {criteria} is unuseful\")\n",
        "        continue\n",
        "      gain = information_gain(true_rows , true_labels , false_rows , false_labels , current_impurity , criterion) \n",
        "      if gain >= best_gain:\n",
        "        best_gain = gain\n",
        "        best_feature_id = f\n",
        "        best_value = criteria\n",
        "        best_isbinary = is_binary\n",
        "  return best_feature_id , best_value , best_gain , best_isbinary\n",
        "  \n",
        "def is_leaf(depth , max_depth , best_gain , expected = None): \n",
        "  if best_gain == 0:\n",
        "    return True\n",
        "  if expected != None:\n",
        "    if best_gain < expected:\n",
        "      return True\n",
        "  if max_depth != None:\n",
        "    if depth == max_depth:\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "qd-LBVioltnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decision_Node():\n",
        "  def __init__(self , question_col , threshold , true_branch , false_branch , depth , isbinary):\n",
        "      self.question_col = question_col\n",
        "      self.threshold = threshold\n",
        "      self.true_branch = true_branch\n",
        "      self.false_branch = false_branch\n",
        "      self.depth = depth\n",
        "      self.isbinary = isbinary\n",
        "\n",
        "class Leaf_Node():\n",
        "  def __init__(self , zero_prob , one_prob , depth):\n",
        "      self.zero_prob = zero_prob\n",
        "      self.one_prob = one_prob\n",
        "      self.depth = depth"
      ],
      "metadata": {
        "id": "OcXmiMokanwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7uyVrLRZC-G"
      },
      "outputs": [],
      "source": [
        "class DecisionTree():\n",
        "  def __init__(self, criterion='gini', max_depth = None , random_ = 0):\n",
        "      self.criterion = criterion\n",
        "      self.max_depth = max_depth\n",
        "      self.random_ = random_\n",
        "      \n",
        "      self.root_node = None\n",
        "      self.feature_importance = {}\n",
        "\n",
        "  def fit(self, x_data, y_data): \n",
        "    global X\n",
        "    global Y\n",
        "    X = x_data\n",
        "    Y = y_data\n",
        "    rows_till_now = [i for i in range(len(x_data))]\n",
        "    labels_till_now = y_data\n",
        "    if self.random_ == 0:\n",
        "      self.random_ = x_data.shape[1]\n",
        "    self.root_node = self.build_tree(rows_till_now , labels_till_now , 0)\n",
        "         \n",
        "  def predict(self, x_data):\n",
        "      predictions = []\n",
        "      for i in range(len(x_data)):\n",
        "        pred = self.traverse(self.root_node , x_data[i])\n",
        "        predictions.append(pred)\n",
        "      return np.array(predictions)\n",
        "\n",
        "  def build_tree(self , rows_till_now , labels_till_now , depth) :\n",
        "      global X\n",
        "      best_feature_id , best_value , best_gain , best_isbinary = find_best_split(\n",
        "      rows_till_now , labels_till_now , self.criterion , X.shape[1] , self.random_)\n",
        "      if is_leaf(depth , self.max_depth , best_gain) : # if it is a leaf node condition , create a leaf_node and then return its reference to its parent\n",
        "        is_zero = 0\n",
        "        is_one = 0\n",
        "        for label in labels_till_now:\n",
        "          if label : is_one += 1\n",
        "          else : is_zero += 1\n",
        "        zero_prob = is_zero/(is_zero + is_one)*1.0\n",
        "        one_prob = 1 - zero_prob\n",
        "        return Leaf_Node(zero_prob , one_prob , depth)\n",
        "      \n",
        "      true_rows , true_labels , false_rows , false_labels = partition(rows_till_now , labels_till_now , best_feature_id , best_value , best_isbinary) \n",
        "      true_branch = self.build_tree(true_rows , true_labels , depth+1) # extend true branch\n",
        "      false_branch = self.build_tree(false_rows , false_labels , depth+1) # extend false branch\n",
        "      return Decision_Node(best_feature_id , best_value , true_branch, false_branch , depth , best_isbinary) # create a decision_node\n",
        "    \n",
        "  def traverse(self , cur_node , a_row_x_data):  # determine a row of data belongs to where\n",
        "      if isinstance(cur_node , Leaf_Node): \n",
        "        return int(cur_node.one_prob > cur_node.zero_prob)\n",
        "      \n",
        "      question_col = cur_node.question_col \n",
        "      threshold = cur_node.threshold\n",
        "      input = a_row_x_data[question_col]\n",
        "      if match(input , question_col , threshold , cur_node.isbinary):\n",
        "        return self.traverse(cur_node.true_branch , a_row_x_data)\n",
        "      else:\n",
        "        return self.traverse(cur_node.false_branch , a_row_x_data)\n",
        "\n",
        "  def DFS(self , cur_node):\n",
        "    if isinstance(cur_node , Leaf_Node) :\n",
        "      return \n",
        "    if self.feature_importance.get(cur_node.question_col) == None:\n",
        "      self.feature_importance[cur_node.question_col] = 1\n",
        "    else:\n",
        "      self.feature_importance[cur_node.question_col] += 1\n",
        "    self.DFS(cur_node.true_branch)\n",
        "    self.DFS(cur_node.false_branch)\n",
        "\n",
        "  def print_tree(self , node, spacing=\"\"):\n",
        "    # Base case: we've reached a leaf\n",
        "    if isinstance(node, Leaf_Node):\n",
        "        print (spacing +\"is leaf\")\n",
        "        return\n",
        "\n",
        "    # Print the question at this node\n",
        "    print (spacing + str(node.question_col) + ' ' + str(node.threshold) , node.depth)\n",
        "\n",
        "    # Call this function recursively on the true branch\n",
        "    print (spacing + '--> True:')\n",
        "    self.print_tree(node.true_branch, spacing + \"  \")\n",
        "\n",
        "    # Call this function recursively on the false branch\n",
        "    print (spacing + '--> False:')\n",
        "    self.print_tree(node.false_branch, spacing + \"  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj3zhGZTZC-G"
      },
      "source": [
        "### Question 2.1\n",
        "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2G6RYb2ZC-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057859df-5072-408f-925e-1db4ab2d5594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using clf_depth3 on validation data :  0.92\n",
            "Accuracy using clf_depth10 on validation data :  0.9433333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
        "\n",
        "clf_depth3.fit(X , Y)\n",
        "predictions = clf_depth3.predict(X_test)\n",
        "print(\"Accuracy using clf_depth3 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier(max_depth = 3)\n",
        "# clf = clf.fit(X, Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_depth3 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "clf_depth10.fit(X , Y)\n",
        "predictions = clf_depth10.predict(X_test)\n",
        "print(\"Accuracy using clf_depth10 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier(max_depth = 10)\n",
        "# clf = clf.fit(X, Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_depth10 on validation data : \" , accuracy_score(Y_test , predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqKk4a2JZC-H"
      },
      "source": [
        "### Question 2.2\n",
        "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrettMbwZC-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe1acd6-3235-429a-cb60-f1a08a5df9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using clf_gini on validation data :  0.92\n",
            "Accuracy using clf_entropy on validation data :  0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
        "\n",
        "clf_gini.fit(X , Y)\n",
        "predictions = clf_gini.predict(X_test)\n",
        "print(\"Accuracy using clf_gini on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier(criterion = 'gini' , max_depth = 3)\n",
        "# clf = clf.fit(X, Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_gini on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "clf_entropy.fit(X , Y)\n",
        "predictions = clf_entropy.predict(X_test)\n",
        "print(\"Accuracy using clf_entropy on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier(criterion = 'entropy' , max_depth = 3)\n",
        "# clf = clf.fit(X, Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_entropy on validation data : \" , accuracy_score(Y_test , predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAAomqWmZC-H"
      },
      "source": [
        "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
        "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
        "- Hint: You can use the recursive method to build the nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EufzA5ssZC-I"
      },
      "source": [
        "## Question 3\n",
        "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
        "\n",
        "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
        "\n",
        "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "clf_depth10.DFS(clf_depth10.root_node)\n",
        "fimportance = clf_depth10.feature_importance\n",
        "# print(fimportance)\n",
        "features = list(fimportance.keys())\n",
        "feature_names_ = []\n",
        "for i in range(len(features)):\n",
        "  feature_names_.append(feature_names[features[i]])\n",
        "time = list(fimportance.values())\n",
        "plt.barh(feature_names_ , time)\n",
        "plt.ylabel('Feature')\n",
        "plt.xlabel('number of time to use this feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U8qrNSQzjHgI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "18d63d5b-bfee-484b-fdd6-28f6e4cd45b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqqkf3JeZC-I"
      },
      "source": [
        "## Question 4\n",
        "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
        "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9udNdJ4ZC-I"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class AdaBoost():\n",
        "    def __init__(self, n_estimators , max_depth = None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.classifier_weight = []\n",
        "        self.each_tree_max_depth = max_depth\n",
        "        self.roots = []\n",
        "    \n",
        "    def fit(self, x_data, y_data):\n",
        "      global X\n",
        "      global Y\n",
        "\n",
        "      X = x_data\n",
        "      Y = y_data\n",
        "\n",
        "      x_origin = X\n",
        "      y_origin = Y\n",
        "\n",
        "      data_weight_ = []\n",
        "      for i in range(len(x_data)):\n",
        "        data_weight_.append(1.0 * 1/x_data.shape[0])\n",
        "    \n",
        "      for t in range(self.n_estimators):\n",
        "        # print(prev == data_weight_)\n",
        "        best_weak_classifier = DecisionTree(max_depth = self.each_tree_max_depth)\n",
        "        chosen_rows = np.random.choice(len(x_data), len(x_data), p = data_weight_)\n",
        "        xx_data = []\n",
        "        yy_data = []\n",
        "        for row in chosen_rows:\n",
        "          xx_data.append(x_data[row])\n",
        "          yy_data.append(y_data[row])\n",
        "\n",
        "        best_weak_classifier.fit(np.array(xx_data) , yy_data)\n",
        "        self.roots.append(best_weak_classifier.root_node)\n",
        "        # print(best_weak_classifier.root_node.question , best_weak_classifier.root_node.threshold)\n",
        "        predictions = best_weak_classifier.predict(x_data)\n",
        "        weighted_err = 0\n",
        "        for i , pred in enumerate(predictions):\n",
        "          if pred != y_data[i]:\n",
        "            weighted_err += data_weight_[i]\n",
        "        self.classifier_weight.append( 0.5 * np.log( (1-weighted_err) / (weighted_err + 1e-9)) )\n",
        "        \n",
        "\n",
        "        for i in range(len(data_weight_)):\n",
        "          if y_data[i] == predictions[i]:\n",
        "            data_weight_[i] *= math.exp(-1.0 * self.classifier_weight[t])\n",
        "          else :\n",
        "            data_weight_[i] *= math.exp(1.0 * self.classifier_weight[t])\n",
        "        sum_ = sum(data_weight_)\n",
        "        for i , w in enumerate(data_weight_):\n",
        "          data_weight_[i] = 1.0 * w/sum_\n",
        "\n",
        "        X = x_origin\n",
        "        Y = y_origin\n",
        "                \n",
        "\n",
        "    def traverse(self , cur_node , a_row_x_data):  # determine a row of data belongs to where\n",
        "      if isinstance(cur_node , Leaf_Node): \n",
        "        return int(cur_node.one_prob >= cur_node.zero_prob)\n",
        "      \n",
        "      question_col = cur_node.question_col \n",
        "      threshold = cur_node.threshold\n",
        "      input = a_row_x_data[question_col]\n",
        "      if match(input , question_col , threshold , cur_node.isbinary):\n",
        "        return self.traverse(cur_node.true_branch , a_row_x_data)\n",
        "      else:\n",
        "        return self.traverse(cur_node.false_branch , a_row_x_data)\n",
        "\n",
        "    def predict(self, x_data):\n",
        "      predictions = []\n",
        "      for i in range(x_data.shape[0]):\n",
        "        Hofx = 0.0\n",
        "        for t , root in enumerate(self.roots):\n",
        "          pred = self.traverse(root , x_data[i])  #定義label 0 給定 -1 ; label 1 給定 +1\n",
        "          if pred : \n",
        "            Hofx += 1.0 * self.classifier_weight[t]\n",
        "          else : \n",
        "            Hofx += -1.0 * self.classifier_weight[t]\n",
        "        predictions.append(int(Hofx > 0.0))\n",
        "        # print(f\"Total {self.n_estimators} , {is_0} says 0 ; {is_1} says 1 , \")\n",
        "      return np.array(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_exMjjJZC-I"
      },
      "source": [
        "### Question 4.1\n",
        "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNgzAZr2ZC-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac28c5d-1402-40eb-ad58-c542d5963cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using clf_adaboost10 on validation data :  0.91\n",
            "Accuracy using clf_adaboost100 on validation data :  0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "clf_adaboost10 = AdaBoost(n_estimators = 10 , max_depth = 1)\n",
        "clf_adaboost100 = AdaBoost(n_estimators = 100 , max_depth = 1)\n",
        "\n",
        "clf_adaboost10.fit(X , Y)\n",
        "predictions = clf_adaboost10.predict(X_test)\n",
        "print(\"Accuracy using clf_adaboost10 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "\n",
        "# clf = AdaBoostClassifier(n_estimators=10)\n",
        "# clf.fit(X , Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_adaboost10 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "clf_adaboost100.fit(X , Y)\n",
        "predictions = clf_adaboost100.predict(X_test)\n",
        "print(\"Accuracy using clf_adaboost100 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = AdaBoostClassifier(n_estimators=100)\n",
        "# clf.fit(X , Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_adaboost100 on validation data : \" , accuracy_score(Y_test , predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R60RPEzSZC-I"
      },
      "source": [
        "## Question 5\n",
        "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
        "\n",
        "1. **n_estimators**: The number of trees in the forest. \n",
        "2. **max_features**: The number of random select features to consider when looking for the best split\n",
        "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8yozoyjZC-J"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class RandomForest():\n",
        "    def __init__(self, n_estimators, max_features, boostrap = True, criterion='gini', max_depth=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = int(max_features)\n",
        "        self.boostrap = boostrap\n",
        "        self.criterion = criterion\n",
        "        self.each_tree_max_depth = max_depth\n",
        "        self.roots = []\n",
        "\n",
        "    def Bagging(self , x_data , y_data):\n",
        "      if not self.boostrap:\n",
        "        return np.array(x_data) , np.array(y_data)\n",
        "      bag_x_data = []\n",
        "      bag_y_data = []\n",
        "      for i in range(len(x_data)):\n",
        "        picked = random.randint(0,len(x_data)-1)\n",
        "        bag_x_data.append(x_data[picked])\n",
        "        bag_y_data.append(y_data[picked])\n",
        "      return np.array(bag_x_data) , np.array(bag_y_data)\n",
        "    \n",
        "    def Create_decision_tree(self , x_data , y_data):\n",
        "      clf = DecisionTree(criterion=self.criterion , max_depth = self.each_tree_max_depth , random_ = self.max_features)\n",
        "      clf.fit(x_data , y_data)\n",
        "      self.roots.append(clf.root_node)\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "        global X\n",
        "        global Y\n",
        "\n",
        "        X = x_data\n",
        "        Y = y_data\n",
        "\n",
        "        x_origin = X\n",
        "        y_origin = Y\n",
        "\n",
        "        for time in range(self.n_estimators):\n",
        "          ax_data , ay_data = self.Bagging(x_data , y_data)\n",
        "          self.Create_decision_tree(ax_data , ay_data)\n",
        "          X = x_origin\n",
        "          Y = y_origin\n",
        "        \n",
        "    def predict(self, x_data):\n",
        "        predictions = []\n",
        "        for i in range(len(x_data)):\n",
        "          is_one = 0\n",
        "          is_zero = 0\n",
        "          for root in self.roots:\n",
        "            pred = self.traverse(root , x_data[i])\n",
        "            if pred : is_one+=1\n",
        "            else : is_zero+=1\n",
        "          predictions.append(int(is_one > is_zero))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def traverse(self , cur_node , a_row_x_data):  # determine a row of data belongs to where\n",
        "      if isinstance(cur_node , Leaf_Node): \n",
        "        return int(cur_node.one_prob > cur_node.zero_prob)\n",
        "      \n",
        "      question_col = cur_node.question_col \n",
        "      threshold = cur_node.threshold\n",
        "      input = a_row_x_data[question_col]\n",
        "      if match(input , question_col , threshold , cur_node.isbinary):\n",
        "        return self.traverse(cur_node.true_branch , a_row_x_data)\n",
        "      else:\n",
        "        return self.traverse(cur_node.false_branch , a_row_x_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_fLssCZC-J"
      },
      "source": [
        "### Question 5.1\n",
        "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H72U5LtsZC-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7524fcc6-170a-428f-c68d-3d5b45ab5dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using clf_10 on validation data :  0.9333333333333333\n",
            "Accuracy using clf_100 on validation data :  0.9366666666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf_10tree = RandomForest(n_estimators=10, max_features = np.sqrt(len(feature_names)))\n",
        "clf_100tree = RandomForest(n_estimators=100, max_features = np.sqrt(len(feature_names)))\n",
        "\n",
        "\n",
        "clf_10tree.fit(X , Y)\n",
        "predictions = clf_10tree.predict(X_test)\n",
        "print(\"Accuracy using clf_10 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "clf_100tree.fit(X , Y)\n",
        "predictions = clf_100tree.predict(X_test)\n",
        "print(\"Accuracy using clf_100 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = RandomForestClassifier(n_estimators=10)\n",
        "# clf.fit(X, Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn clf_10 on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "# clf = RandomForestClassifier(n_estimators=100)\n",
        "# clf.fit(X, Y)\n",
        "# predictions = clf.predict(X_test)\n",
        "# print(\"Accuracy using sklearn sklearn clf_100 on validation data : \" , accuracy_score(Y_test , predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olxn9pitZC-J"
      },
      "source": [
        "### Question 5.2\n",
        "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMbrIisIZC-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5f3008-21d5-4cec-b05a-bcd3bed76835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using clf_random_features on validation data :  0.9133333333333333\n",
            "Accuracy using clf_all_features on validation data :  0.9566666666666667\n"
          ]
        }
      ],
      "source": [
        "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(len(feature_names)))\n",
        "clf_all_features = RandomForest(n_estimators=10, max_features=len(feature_names))\n",
        "\n",
        "clf_random_features.fit(X , Y)\n",
        "predictions = clf_random_features.predict(X_test)\n",
        "print(\"Accuracy using clf_random_features on validation data : \" , accuracy_score(Y_test , predictions))\n",
        "\n",
        "clf_all_features.fit(X , Y)\n",
        "predictions = clf_all_features.predict(X_test)\n",
        "print(\"Accuracy using clf_all_features on validation data : \" , accuracy_score(Y_test , predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8wP1BOQZC-K"
      },
      "source": [
        "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J0f9wBiZC-K"
      },
      "source": [
        "### Question 6. Train and tune your model on a real-world dataset\n",
        "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
        "- Feature engineering\n",
        "- Hyperparameter tuning\n",
        "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPNUzLkfZC-K"
      },
      "outputs": [],
      "source": [
        "def train_your_model(data): \n",
        "  '''\n",
        "   因為用global寫 , 所以重複執行某個cell block會導致資料集不一致(或是index out of range等)!!!\n",
        "   請重頭跑到尾run all一次整份程式即可。\n",
        "\n",
        "   在這整份作業中我對資料集的操作都是用global的原因是避免資料要在object\n",
        "   中傳遞會每次都要copy一份 , 這樣很耗時間 , 所以乾脆利用程式執行順序去每次改一個全域變數 , 不同object創立後都會去看同一個名字的全域變數 , 但是不同時間點這個變數的資料集內容會不同\n",
        "   ex:在adaboost中每一輪因為要依據各筆資料的weight sample 原本丟進fit的原始資料集 , 所以可能在每一個找weak classifier的round時 , 這份全域資料集X,Y會先變成那份新sample的資料集以提供給各函數做操作 , 在結束\n",
        "   此輪後又會把這個全域資料集X,Y改回丟進fit的原始資料集 , 這樣下一輪又要根據各筆資料的weight sample時 , 看的才會是同一份丟進fit裡面的原始資料集\n",
        "\n",
        "   而在每次fit的時候都會用傳入的data改寫那個全域變數 , 所以可以保證正確的程式執行\n",
        "  '''\n",
        "  global X\n",
        "  global Y\n",
        "\n",
        "  XplusXtest = []\n",
        "  YplusYtest = []\n",
        "  for i in X:\n",
        "    XplusXtest.append(i)\n",
        "  for i in Y:\n",
        "    YplusYtest.append(i)\n",
        "  for i in X_test:\n",
        "    XplusXtest.append(i)\n",
        "  for i in Y_test:\n",
        "    YplusYtest.append(i)\n",
        "\n",
        "  clf_adaboost150 = AdaBoost(n_estimators = 150 , max_depth = 1)\n",
        "  # print(np.array(XplusXtest).shape , np.array(YplusYtest).shape)\n",
        "  clf_adaboost150.fit(np.array(XplusXtest) , np.array(YplusYtest))\n",
        "  return clf_adaboost150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvuqDie_ZC-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "39da2b5d-9ca0-45e7-cccb-92ca37f93127"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f0c6e1c01ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_your_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-f19ffc34af9a>\u001b[0m in \u001b[0;36mtrain_your_model\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mclf_adaboost150\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# print(np.array(XplusXtest).shape , np.array(YplusYtest).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mclf_adaboost150\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXplusXtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYplusYtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mclf_adaboost150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-228887fe2982>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_data, y_data)\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0myy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mbest_weak_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0myy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_weak_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# print(best_weak_classifier.root_node.question , best_weak_classifier.root_node.threshold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9b48fa0ef03b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_data, y_data)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_till_now\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels_till_now\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9b48fa0ef03b>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(self, rows_till_now, labels_till_now, depth)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mtrue_rows\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfalse_rows\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfalse_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_till_now\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels_till_now\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest_feature_id\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest_value\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest_isbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mtrue_branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_rows\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# extend true branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       \u001b[0mfalse_branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_rows\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfalse_labels\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# extend false branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mDecision_Node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_feature_id\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest_value\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrue_branch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_branch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest_isbinary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create a decision_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9b48fa0ef03b>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(self, rows_till_now, labels_till_now, depth)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mglobal\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       best_feature_id , best_value , best_gain , best_isbinary = find_best_split(\n\u001b[0;32m---> 31\u001b[0;31m       rows_till_now , labels_till_now , self.criterion , X.shape[1] , self.random_)\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m# if it is a leaf node condition , create a leaf_node and then return its reference to its parent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mis_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-eaf0b7111bc0>\u001b[0m in \u001b[0;36mfind_best_split\u001b[0;34m(rows_till_now, labels_till_now, criterion, total_features, random_)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mcriteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mtrue_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfalse_rows\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfalse_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_till_now\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels_till_now\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mis_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;31m# print(len(true_rows) , len(false_rows))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_rows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_rows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 無用的拆分 , 意即根本沒辦法分群的一個條件組合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-eaf0b7111bc0>\u001b[0m in \u001b[0;36mpartition\u001b[0;34m(rows_till_now, labels_till_now, col, criteria, is_binary)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_till_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mis_binary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m       \u001b[0mtrue_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_till_now\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-eaf0b7111bc0>\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(input, col, threshold, is_binary)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mis_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 對於第col個的feature , 檢驗input值是否符合threshold , 丟入col的原因是為了檢察是不是binary column(either 0 or 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_binary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "my_model = train_your_model(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG8YOz4pZC-K"
      },
      "outputs": [],
      "source": [
        "y_pred = my_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsQ3skmIZC-K"
      },
      "outputs": [],
      "source": [
        "assert y_pred.shape == (500, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_ggXXULZC-L"
      },
      "source": [
        "## Supplementary\n",
        "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a6IdRNiZC-L"
      },
      "source": [
        "### DO NOT MODIFY CODE BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m3CK8JjZC-L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
        "\n",
        "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyPmeHQLZC-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U6D2bKLZC-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fe0408-b07f-47a3-dbfb-fd2a6e047ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** We will check your result for Question 3 manually *** (5 points)\n",
            "*** We will check your result for Question 6 manually *** (20 points)\n",
            "Approximate score range: 45.0 ~ 70.0\n",
            "*** This score is only for reference ***\n"
          ]
        }
      ],
      "source": [
        "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "        return score\n",
        "    else:\n",
        "        print(f\"{name} failed\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def patient_checker(score, thres, CLS, kwargs, name,\n",
        "                    x_train, y_train, x_test, y_test, patient=10):\n",
        "    while patient > 0:\n",
        "        patient -= 1\n",
        "        clf = CLS(**kwargs)\n",
        "        clf.fit(x_train, y_train)\n",
        "        y_pred = clf.predict(x_test)\n",
        "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "            return score\n",
        "    print(f\"{name} failed\")\n",
        "    print(\"Considering the randomness, we will check it manually\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
        "    df = pd.read_csv(\n",
        "        file_url,\n",
        "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
        "    )\n",
        "\n",
        "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
        "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
        "\n",
        "    train_idx = range(0, len(df), 10)\n",
        "    test_idx = range(1, len(df), 20)\n",
        "\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    feature_names = x_train.columns.values\n",
        "    x_train = x_train.values\n",
        "    y_train = train_df['Target'].values\n",
        "\n",
        "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    x_test = x_test.values\n",
        "    y_test = test_df['Target'].values\n",
        "    return x_train, y_train, x_test, y_test, feature_names\n",
        "\n",
        "\n",
        "score = 0\n",
        "\n",
        "data = np.array([1, 2])\n",
        "if abs(gini(data) - 0.5) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"gini test failed\")\n",
        "\n",
        "if abs(entropy(data) - 1) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"entropy test failed\")\n",
        "\n",
        "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
        "\n",
        "score += discrete_checker(5, 0.9337,\n",
        "                          DecisionTree(criterion='gini', max_depth=3),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9036,\n",
        "                          DecisionTree(criterion='gini', max_depth=10),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9096,\n",
        "                          DecisionTree(criterion='entropy', max_depth=3),\n",
        "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
        "    \"AdaBoost(n_estimators=10)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
        "    \"AdaBoost(n_estimators=100)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.92, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
        "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
        "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
        "print(\"*** This score is only for reference ***\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c9afcc29181526987d0b385fb14ac7bec129a3a1f159e207d0db8e842e65205"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}